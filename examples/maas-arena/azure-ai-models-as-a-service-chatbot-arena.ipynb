{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Models-as-a-Service (MaaS) Chatbot Arena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialization\n",
    "\n",
    "To ensure that all necessary libraries and packages are installed for the project, we have created a `requirements.txt` file.\n",
    "\n",
    "To install the dependencies listed in the `requirements.txt` file, run the following command in your terminal:\n",
    "\n",
    "```sh\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import gradio as gr\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from maas_clients import initialize_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-processing\n",
    "\n",
    "The MMLU (Massive Multitask Language Understanding) dataset is a comprehensive benchmark designed to evaluate the performance of language models across a wide range of tasks and subjects. It includes questions from various domains such as humanities, social sciences, STEM, and more, making it a diverse and challenging dataset for assessing the generalization capabilities of language models.\n",
    "\n",
    "To load the MMLU dataset, we use the `load_dataset` function from the `datasets` library. This dataset includes various splits such as 'test', 'validation', and 'dev'. We extract a distinct list of all subjects in the dataset, representing the different domains and topics covered by the MMLU dataset.\n",
    "\n",
    "Examples are generated based on the selected subjects. An interactive interface (e.g., using Gradio) is set up to allow users to input questions, select models, and view the responses in real-time. This interface also includes options to clear the chat history, generate new examples, and adjust model parameters like temperature and max tokens.\n",
    "\n",
    "By following these steps, the MMLU dataset is effectively used to generate a wide range of examples, enabling a comprehensive comparison of the output, quality, and performance of different LLMs side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MMLU dataset from the \"cais/mmlu\" repository\n",
    "ds = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# Convert the 'test', 'validation', and 'dev' splits of the dataset into DataFrames\n",
    "test_df = pd.DataFrame(ds['test'])\n",
    "validation_df = pd.DataFrame(ds['validation'])\n",
    "dev_df = pd.DataFrame(ds['dev'])\n",
    "\n",
    "# Optionally, concatenate all DataFrames into one combined DataFrame\n",
    "combined_df = pd.concat([test_df, validation_df, dev_df], ignore_index=True)\n",
    "\n",
    "# Extract a distinct list of all subjects in the 'subject' column and store it in a variable\n",
    "subjects_array = combined_df['subject'].unique()\n",
    "\n",
    "# Convert the array of subjects to a list\n",
    "subjects = subjects_array.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Initialization\n",
    "\n",
    "The code snippet initializes clients for various language models using the `initialize_clients` function. This function returns a dictionary where each key corresponds to a different language model client. These clients can be accessed using their respective keys from the dictionary.\n",
    "\n",
    "The `maas_clients.py` file is a vital part of our project, responsible for setting up and managing these language model clients. It contains the `initialize_clients` function, which establishes connections to different language models and returns a dictionary of clients. Each client in the dictionary represents a specific language model, enabling seamless interaction with multiple models.\n",
    "\n",
    "This setup simplifies access and interaction with various language models, making it easier to generate responses, compare model outputs, and evaluate performance across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = initialize_clients()\n",
    "\n",
    "# Access clients like this\n",
    "gpt_4o_client = clients[\"gpt_4o_client\"]\n",
    "gpt_4_turbo_client = clients[\"gpt_4_turbo_client\"]\n",
    "jamba_instruct_client = clients[\"jamba_instruct_client\"]\n",
    "command_r_client = clients[\"command_r_client\"]\n",
    "command_r_plus_client = clients[\"command_r_plus_client\"]\n",
    "jais_30b_client = clients[\"jais_30b_client\"]\n",
    "llama_3_1_405B_client = clients[\"llama_3_1_405B_client\"]\n",
    "llama_3_1_70B_client = clients[\"llama_3_1_70B_client\"]\n",
    "llama_3_1_8B_client = clients[\"llama_3_1_8B_client\"]\n",
    "mistral_large_client = clients[\"mistral_large_client\"]\n",
    "mistral_large_2407_client = clients[\"mistral_large_2407_client\"]\n",
    "mistral_nemo_client = clients[\"mistral_nemo_client\"]\n",
    "phi_3_medium_128k_instruct_client = clients[\"phi_3_medium_128k_instruct_client\"]\n",
    "phi_3_small_128k_instruct_client = clients[\"phi_3_small_128k_instruct_client\"]\n",
    "phi_3_mini_4k_instruct_client = clients[\"phi_3_mini_4k_instruct_client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS and HTML\n",
    "In this section of the notebook, we set up the visual and structural components necessary for creating an interactive interface for the Azure AI Models-as-a-Service (MaaS) Arena. This involves defining CSS for styling, and creating an HTML title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    "h1 {\n",
    "  margin: 0;\n",
    "  flex-grow: 1;\n",
    "  font-size: 24px;\n",
    "  min-width: 200px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "title = \"\"\"<h1 style=\"text-align: center;\">Welcome to the Azure AI Models-as-a-Service (MaaS) Arena</h1>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and model selection functions\n",
    "This section of the notebook provides functions for handling inference and model selection for chatbot interactions. The key components include:\n",
    "\n",
    "1. `user_model` Function:\n",
    "    - This function takes a user message and the current chat history as inputs.\n",
    "    - It returns an empty string and updates the chat history by appending the user message with a placeholder for the assistant's response.\n",
    "2. `chat_model_1` and `chat_model_2` Functions:\n",
    "    - Both functions are designed to handle the chatbot's response generation.\n",
    "    - They take the chat history, temperature, maximum tokens, and model name as inputs.\n",
    "    - They select the appropriate model client using the `select_model` function.\n",
    "    - They convert the chat history into a format suitable for the model.\n",
    "    - They generate responses from the model in a streaming fashion, updating the chat history with the assistant's responses.\n",
    "3. `select_model` Function:\n",
    "    - This function maps the provided model name to the corresponding model client.\n",
    "    - It supports various models such as GPT-4o, GPT-4 Turbo, AI21 Jamba-Instruct, and others.\n",
    "    - If an unknown model name is provided, it raises a ValueError.\n",
    "\n",
    "These functions collectively enable the notebook to perform dynamic model selection and generate responses for chatbot interactions, facilitating experimentation with different models and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference functions for Chatbots \n",
    "def user_model(user_message, chat_history):\n",
    "    return \"\", chat_history + [[user_message, None]]\n",
    "\n",
    "# Modified chat_model_1\n",
    "def chat_model_1(chat_history, temp, max_tokens, model_name):\n",
    "    selected_client = select_model(model_name)\n",
    "    chat_history[-1][1] = \"\"\n",
    "\n",
    "    # Convert chat history to message dictionaries\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, assistant in chat_history[:-1]:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        if assistant is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": chat_history[-1][0]})\n",
    "\n",
    "    response = selected_client.complete(\n",
    "        stream=True,\n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    for update in response:\n",
    "        if update.choices and update.choices[0].delta.content is not None:\n",
    "            chat_history[-1][1] += update.choices[0].delta.content or \"\"\n",
    "            yield chat_history\n",
    "\n",
    "    yield chat_history\n",
    "\n",
    "# Modified chat_model_2\n",
    "def chat_model_2(chat_history, temp, max_tokens, model_name):\n",
    "    selected_client = select_model(model_name)\n",
    "    chat_history[-1][1] = \"\"\n",
    "\n",
    "    # Convert chat history to message dictionaries\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, assistant in chat_history[:-1]:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        if assistant is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": chat_history[-1][0]})\n",
    "\n",
    "    response = selected_client.complete(\n",
    "        stream=True,\n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    for update in response:\n",
    "        if update.choices and update.choices[0].delta.content is not None:\n",
    "            chat_history[-1][1] += update.choices[0].delta.content or \"\"\n",
    "            yield chat_history\n",
    "\n",
    "    yield chat_history\n",
    "\n",
    "def select_model(model_name):\n",
    "    if model_name == \"GPT-4o\":\n",
    "        return gpt_4o_client\n",
    "    elif model_name == \"GPT-4 Turbo\":\n",
    "        return gpt_4_turbo_client\n",
    "    elif model_name == \"AI21 Jamba-Instruct\":\n",
    "        return jamba_instruct_client\n",
    "    elif model_name == \"Cohere Command R\":\n",
    "        return command_r_client\n",
    "    elif model_name == \"Cohere Command R+\":\n",
    "        return command_r_plus_client\n",
    "    elif model_name == \"Jais 30B\":\n",
    "        return jais_30b_client\n",
    "    elif model_name == \"Llama3.1 405B\":\n",
    "        return llama_3_1_405B_client\n",
    "    elif model_name == \"Llama3.1 70B\":\n",
    "        return llama_3_1_70B_client\n",
    "    elif model_name == \"Llama3.1 8B\":\n",
    "        return llama_3_1_8B_client\n",
    "    elif model_name == \"Mistral-Large\":\n",
    "        return mistral_large_client\n",
    "    elif model_name == \"Mistral-Large-2407\":\n",
    "        return mistral_large_2407_client\n",
    "    elif model_name == \"Mistral-Nemo\":\n",
    "        return mistral_nemo_client\n",
    "    elif model_name == \"Phi-3-medium-128k\":\n",
    "        return phi_3_medium_128k_instruct_client\n",
    "    elif model_name == \"Phi-3-small-128k\":\n",
    "        return phi_3_small_128k_instruct_client\n",
    "    elif model_name == \"Phi-3-mini-4k\":\n",
    "        return phi_3_mini_4k_instruct_client\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU Example Handling Functions and Gradio Interface\n",
    "\n",
    "This section of the notebook sets up an interactive Gradio interface for experimenting with different chatbot models and configurations. The key components include:\n",
    "\n",
    "1. **Dropdowns for Model Selection**:\n",
    "    - Two dropdowns (`model_dropdown1` and `model_dropdown2`) allow users to select different models for comparison.\n",
    "    - The available models include GPT-4o, GPT-4 Turbo, AI21 Jamba-Instruct, Cohere Command R, and others.\n",
    "\n",
    "2. **Chatbot Interfaces**:\n",
    "    - Two chatbot interfaces (`chatbot1` and `chatbot2`) display the conversation history for the selected models.\n",
    "    - Each chatbot has a placeholder indicating no messages yet and a label showing the selected model.\n",
    "\n",
    "3. **User Input and Control Buttons**:\n",
    "    - A textbox (`user_msg`) for users to input their messages.\n",
    "    - A submit button (`submit_button`) to send the message.\n",
    "    - A clear button (`clear_button`) to clear the chat history.\n",
    "\n",
    "4. **Additional Parameters**:\n",
    "    - An accordion (`additional_inputs_accordion`) for additional settings like temperature and maximum tokens.\n",
    "    - A slider (`temperature`) to adjust the temperature of the model.\n",
    "    - A slider (`max_tokens`) to set the maximum number of tokens for the model's response.\n",
    "\n",
    "5. **Subject and Example Generation**:\n",
    "    - A dropdown (`subject`) to select a subject for generating example questions.\n",
    "    - Buttons (`generate`, `random_button`, `clear`) to generate examples, generate random examples, and revert to cached examples, respectively.\n",
    "    - An examples component (`examples`) to display and select example questions.\n",
    "\n",
    "6. **Event Handling**:\n",
    "    - The `generate` button updates the examples based on the selected subject.\n",
    "    - The `random_button` generates random examples based on the selected subject.\n",
    "    - The `clear` button reverts to the cached examples.\n",
    "    - Clicking on an example updates the user message textbox.\n",
    "\n",
    "7. **Model Selection and Chatbot Update**:\n",
    "    - Selecting a model from the dropdown updates the corresponding chatbot's label.\n",
    "    - The `user_msg` textbox and `submit_button` handle user input and trigger the chatbot response generation using the selected model.\n",
    "\n",
    "8. **Launching the Interface**:\n",
    "    - The Gradio interface is launched with `demo.launch(debug=True)` to enable debugging and interaction.\n",
    "\n",
    "This setup allows users to interactively test and compare different chatbot models, adjust parameters, and generate example questions for various subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chatbot(select_data):\n",
    "    return gr.update(label=f\"{select_data}\")\n",
    "\n",
    "def update_max_tokens(model_name):\n",
    "    if model_name == \"Phi-3-mini-4k\":\n",
    "        return gr.update(maximum=2048, value=min(2048, max_tokens.value))\n",
    "    else:\n",
    "        return gr.update(maximum=4096, value=min(4096, max_tokens.value))\n",
    "    \n",
    "def generate_subject_examples(subject):\n",
    "    # Filter the DataFrame to only include rows where the subject matches the input\n",
    "    subject_df = test_df[test_df['subject'] == subject]\n",
    "    # Convert the selected rows to a list of lists\n",
    "    question_list = [[question] for question in subject_df['question'].head().values.tolist()]\n",
    "    # Return the list of lists\n",
    "    return question_list\n",
    "\n",
    "def generate_subject_random_examples(subject):\n",
    "    # Filter the DataFrame to only include rows where the subject matches the input\n",
    "    subject_df = test_df[test_df['subject'] == subject]\n",
    "    # Generate a random sample of five rows from the filtered DataFrame\n",
    "    question_list = subject_df['question'].sample(n=5).tolist()  # Use random_state for reproducibility\n",
    "    # Convert the selected rows to a list of lists\n",
    "    question_list = [[question] for question in question_list]\n",
    "    # Return the list of lists\n",
    "    return question_list\n",
    "\n",
    "cached_examples = [\n",
    "    [\"There's a llama in my garden üò± What should I do?\"],\n",
    "    [\"What is the best way to open a can of worms?\"],\n",
    "    [\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"],\n",
    "    ['How to setup a human base on Mars? Give short answer.'],\n",
    "    ['Explain theory of relativity to me like I‚Äôm 8 years old.'],\n",
    "    ['What is 9,000 * 9,000?'],\n",
    "    ['Write a pun-filled happy birthday message to my friend Alex.'],\n",
    "    ['Justify why a penguin might make a good king of the jungle.'],\n",
    "    ['Give me 5 good reasons why I should exercise every day.'],\n",
    "    ['How many languages are in the world?']\n",
    "]\n",
    "\n",
    "# Function to handle example click\n",
    "def display_example(example):\n",
    "    return example[1][0]  # Return the first element of the example list\n",
    "\n",
    "# Function to update examples\n",
    "def update_examples(subject):\n",
    "    new_examples = generate_subject_examples(subject)\n",
    "    examples.dataset.samples = new_examples\n",
    "    return gr.Dataset(samples=new_examples)\n",
    "\n",
    "# Function to update random examples\n",
    "def update_random_examples(subject):\n",
    "    new_examples = generate_subject_random_examples(subject)\n",
    "    examples.dataset.samples = new_examples\n",
    "    return gr.Dataset(samples=new_examples)\n",
    "\n",
    "# Function to revert to cached examples\n",
    "def revert_to_cached_examples():\n",
    "    examples.dataset.samples = cached_examples\n",
    "    return gr.Dataset(samples=cached_examples)\n",
    "\n",
    "# Gradio Blocks\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    gr.HTML(title)\n",
    "\n",
    "    with gr.Row():\n",
    "        model_dropdown1 = gr.Dropdown(\n",
    "            choices=[\n",
    "                \"GPT-4o\", \"GPT-4 Turbo\", \"AI21 Jamba-Instruct\", \"Cohere Command R\", \"Cohere Command R+\", \n",
    "                \"Jais 30B\", \"Llama3.1 405B\", \"Llama3.1 70B\", \"Llama3.1 8B\", \"Mistral-Large\", \n",
    "                \"Mistral-Large-2407\", \"Mistral-Nemo\", \"Phi-3-medium-128k\", \"Phi-3-small-128k\", \"Phi-3-mini-4k\"\n",
    "            ],\n",
    "            label=\"Model A\",\n",
    "            value=\"Llama3.1 405B\"\n",
    "        )\n",
    "        model_dropdown2 = gr.Dropdown(\n",
    "            choices=[\n",
    "                \"GPT-4o\", \"GPT-4 Turbo\", \"AI21 Jamba-Instruct\", \"Cohere Command R\", \"Cohere Command R+\", \n",
    "                \"Jais 30B\", \"Llama3.1 405B\", \"Llama3.1 70B\", \"Llama3.1 8B\", \"Mistral-Large\", \n",
    "                \"Mistral-Large-2407\", \"Mistral-Nemo\", \"Phi-3-medium-128k\", \"Phi-3-small-128k\", \"Phi-3-mini-4k\"\n",
    "            ],\n",
    "            label=\"Model B\",\n",
    "            value=\"Mistral-Large\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot1 = gr.Chatbot(\n",
    "            placeholder=\"No messages yet\", \n",
    "            label=\"Llama3.1 405B\"\n",
    "        )\n",
    "        chatbot2 = gr.Chatbot(\n",
    "            placeholder=\"No messages yet\", \n",
    "            label=\"Mistral-Large\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        user_msg = gr.Textbox(placeholder=\"Ask me anything\", label=\"User Messages\", scale=7)\n",
    "        submit_button = gr.Button(\"Send\", variant=\"primary\")   \n",
    "        clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "    additional_inputs_accordion = gr.Accordion(label=\"‚öôÔ∏è Parameters\", open=True)\n",
    "    with additional_inputs_accordion:\n",
    "        temperature = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.90, label=\"Temperature\")\n",
    "        max_tokens = gr.Slider(minimum=128, maximum=4096, step=128, value=2048, label=\"Max tokens\")\n",
    "\n",
    "        # Row for subject and generate button\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    # Subject dropdown and generate button\n",
    "                    subject = gr.Dropdown(choices=subjects, label=\"Subject\")\n",
    "                with gr.Row():\n",
    "                    generate = gr.Button(value=\"Generate\", variant=\"secondary\")\n",
    "                    random_button = gr.Button(value=\"Random\", variant='secondary')\n",
    "                    clear = gr.Button(value=\"Clear\", variant=\"secondary\")\n",
    "            with gr.Column(scale=3):\n",
    "                # Examples component\n",
    "                examples = gr.Examples(examples=cached_examples, inputs=user_msg, label=\"Examples\")           \n",
    "\n",
    "        # Update the examples when a new subject is selected and the generate button is clicked\n",
    "        generate.click(fn=update_examples, inputs=[subject], outputs=examples.dataset)\n",
    "        \n",
    "        # Update the examples when the random button is clicked\n",
    "        random_button.click(fn=update_random_examples, inputs=[subject], outputs=examples.dataset)\n",
    "\n",
    "        # Revert to cached examples when clear button is clicked\n",
    "        clear.click(fn=revert_to_cached_examples, inputs=None, outputs=examples.dataset)\n",
    "        \n",
    "        # Set up the example click event to update the output textbox\n",
    "        examples.dataset.click(fn=display_example, inputs=examples.dataset, outputs=user_msg)\n",
    "            \n",
    "    model_dropdown1.select(\n",
    "        fn=update_chatbot,\n",
    "        inputs=[model_dropdown1], \n",
    "        outputs=[chatbot1],\n",
    "        scroll_to_output=True,\n",
    "        show_progress=\"minimal\"\n",
    "    )\n",
    "\n",
    "    model_dropdown2.select(\n",
    "        fn=update_chatbot,\n",
    "        inputs=[model_dropdown2], \n",
    "        outputs=[chatbot2],\n",
    "        scroll_to_output=True,\n",
    "        show_progress=\"minimal\"\n",
    "    )\n",
    "\n",
    "    # handle the case where the user presses enter instead of clicking the submit button\n",
    "\n",
    "    user_msg.submit(user_model, [user_msg, chatbot1], [user_msg, chatbot1], queue=False).then(\n",
    "        chat_model_1, [chatbot1, temperature, max_tokens, model_dropdown1], [chatbot1]\n",
    "    )\n",
    "    \n",
    "    user_msg.submit(user_model, [user_msg, chatbot2], [user_msg, chatbot2], queue=False).then(\n",
    "        chat_model_2, [chatbot2, temperature, max_tokens, model_dropdown2], [chatbot2]\n",
    "    )\n",
    "\n",
    "    submit_button.click(user_model, [user_msg, chatbot1], [user_msg, chatbot1], queue=False).then(\n",
    "        chat_model_1, [chatbot1, temperature, max_tokens, model_dropdown1], [chatbot1]\n",
    "    )\n",
    "\n",
    "    submit_button.click(user_model, [user_msg, chatbot2], [user_msg, chatbot2], queue=False).then(\n",
    "        chat_model_2, [chatbot2, temperature, max_tokens, model_dropdown2], [chatbot2]\n",
    "    )\n",
    "\n",
    "    clear_button.click(lambda: None, None, chatbot1, queue=False)\n",
    "    clear_button.click(lambda: None, None, chatbot2, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored various chatbot models and their configurations using an interactive Gradio interface. We provided a comprehensive setup that allows users to:\n",
    "\n",
    "- Select and compare different chatbot models.\n",
    "- Adjust parameters such as temperature and maximum tokens to fine-tune the model responses.\n",
    "- Generate and test example questions across different subjects.\n",
    "\n",
    "Through this interactive approach, users can gain insights into the performance and behavior of different models, making it easier to choose the most suitable one for their specific needs. The flexibility and ease of use provided by the Gradio interface enhance the experimentation process, enabling a deeper understanding of the capabilities and limitations of each model.\n",
    "\n",
    "Overall, this notebook serves as a valuable tool for anyone looking to experiment with and evaluate various chatbot models in a user-friendly and interactive manner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
