{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello ~~world~~ Phi-3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "### Importing Required Libraries\n",
    "In this cell, we are importing the necessary libraries. `MLClient` is the main class that we use to interact with Azure AI. `DefaultAzureCredential` and `InteractiveBrowserCredential` are used for authentication purposes. The `os` library is used to access environment variables.\n",
    "\n",
    "```python\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "import os\n",
    "```\n",
    "\n",
    "### Setting Up Credentials\n",
    "Here, we are setting up the credentials to authenticate with Azure. We first try to use the `DefaultAzureCredential`. If that fails (for example, if we are running the code on a machine that is not logged into Azure), we fall back to using `InteractiveBrowserCredential`, which will prompt the user to log in.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "```\n",
    "\n",
    "### Creating an MLClient for Workspace\n",
    "In this cell, we create an `MLClient` for our Azure AI workspace. We use environment variables to get the subscription ID, resource group name, and workspace name.\n",
    "\n",
    "```python\n",
    "workspace_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"WORKSPACE_NAME\"),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "import os\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"WORKSPACE_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Loading and Preparing the Dataset\n",
    "This step is optional and not required to demo Phi-3. However, if you want to experiment with different topics for the model, you can use a dataset. In this case, we are using the `ultrachat_200k` dataset from Hugging Face. First, we import the necessary libraries: `pandas` for data manipulation and `datasets` for loading the dataset.\n",
    "```python\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "```\n",
    "Next, we load the [ultrachat_200k dataset from Hugging Face](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) and select the `test_sft` split.\n",
    "\n",
    "```python\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")[\"test_sft\"]\n",
    "```\n",
    "We then convert the dataset into a pandas DataFrame and drop the 'prompt_id' and 'messages' columns. These columns are not needed for our current task.\n",
    "```python\n",
    "df = pd.DataFrame(dataset).drop(columns=[\"prompt_id\", \"messages\"])\n",
    "```\n",
    "Finally, we display a random sample of 5 rows from the DataFrame. This gives us a quick look at the data we'll be working with.\n",
    "```python\n",
    "df.sample(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'ultrachat_200k' dataset from HuggingFace and select the 'test_sft' split\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")[\"test_sft\"]\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame and drop the 'prompt_id' and 'messages' columns\n",
    "df = pd.DataFrame(dataset).drop(columns=[\"prompt_id\", \"messages\"])\n",
    "\n",
    "# Display a random sample of 5 rows from the DataFrame\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (continued): Selecting a Random Sample\n",
    "In this section, we are selecting a random sample from our dataset to use as a test case for Phi-3. First, we sample 5 random examples from the DataFrame and convert them to a list. This gives us a small set of examples to choose from.\n",
    "```python\n",
    "examples = df.sample(5).values.tolist()\n",
    "```\n",
    "Next, we convert the examples to a JSON string. This is done for pretty printing, which makes the examples easier to read.\n",
    "```python\n",
    "examples_json = json.dumps(examples, indent=2)\n",
    "```\n",
    "We then select a random index from the examples. This is done using the random.randint function, which returns a random integer within the specified range.\n",
    "```python\n",
    "i = random.randint(0, len(examples) - 1)\n",
    "```\n",
    "We use this random index to select an example from our list.\n",
    "```python\n",
    "sample = examples[i]\n",
    "print(sample)\n",
    "```\n",
    "This process ensures that we have a diverse range of topics to test our model with, and that the testing process is as unbiased as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Sample 5 random examples from the DataFrame and convert to list\n",
    "examples = df.sample(5).values.tolist()\n",
    "\n",
    "# Convert the examples to a JSON string for pretty printing\n",
    "examples_json = json.dumps(examples, indent=2)\n",
    "\n",
    "# Select a random index from the examples\n",
    "i = random.randint(0, len(examples) - 1)\n",
    "\n",
    "# Get the example at the random index\n",
    "sample = examples[i]\n",
    "\n",
    "# Print the selected example\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Phi-3 Model\n",
    "In this section, we are invoking the Phi-3 model to generate a response to a user's question.\n",
    "\n",
    "First, we define the input data. This includes the user's message and some parameters for the model. The parameters control the randomness of the model's output.\n",
    "\n",
    "```python\n",
    "messages = {\n",
    "    \"input_data\": { \"input_string\": [ { \"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\" } ],\n",
    "        \"parameters\": { \"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True, \"max_new_tokens\": 1000 } }\n",
    "}\n",
    "```\n",
    "Next, we write the input data to a temporary file. This is necessary because the `invoke` method of the `workspace_ml_client.online_endpoints` object requires a file as input.\n",
    "```python\n",
    "with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode='w') as temp:\n",
    "    json.dump(messages, temp)\n",
    "    temp_file_name = temp.name\n",
    "```\n",
    "We then invoke the Phi-3 model and get the response. The `invoke` method sends the input data to the model and returns the model's output.\n",
    "```python\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=\"aistudio-nbrady-phi-3\",\n",
    "    deployment_name=\"phi-3-mini-4k-instruct\",\n",
    "    request_file=temp_file_name,\n",
    ")\n",
    "```\n",
    "After getting the response, we parse it and add it to the input data. This allows us to keep track of the conversation history. Finally, we print the updated input data. This includes the user's message and the model's response.\n",
    "```python\n",
    "response_json = json.loads(response)[\"output\"]\n",
    "response_dict = {'content': response_json, 'role': 'assistant'}\n",
    "messages['input_data']['input_string'].append(response_dict)\n",
    "print(json.dumps(messages[\"input_data\"][\"input_string\"],indent=2))\n",
    "```\n",
    "This process allows us to interact with the Phi-3 model and get its responses to various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "\n",
    "# Define the input data\n",
    "messages = { \"input_data\": { \n",
    "    \"input_string\": [ { \"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"} ], \n",
    "    \"parameters\": { \"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True, \"max_new_tokens\": 1000, }, } \n",
    "}\n",
    "\n",
    "# Write the input data to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode='w') as temp:\n",
    "    json.dump(messages, temp)\n",
    "    temp_file_name = temp.name\n",
    "\n",
    "# Invoke the Phi-3 model and get the response\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=\"aistudio-nbrady-phi-3\",\n",
    "    deployment_name=\"phi-3-mini-4k-instruct\",\n",
    "    request_file=temp_file_name,\n",
    ")\n",
    "\n",
    "# Parse the response and add it to the input data\n",
    "response_json = json.loads(response)[\"output\"]\n",
    "response_dict = {'content': response_json, 'role': 'assistant'}\n",
    "messages['input_data']['input_string'].append(response_dict)\n",
    "\n",
    "# Print the updated input data\n",
    "print(json.dumps(messages[\"input_data\"][\"input_string\"],indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Sample Input\n",
    "You still with me? Good! In this section, we're doing the same as before, but we are preparing the input data for the Phi-3 model continuing with the sample we created in the previous section.\n",
    "\n",
    "First, we define the input data. This includes the user's message and some parameters for the model. The parameters control the randomness of the model's output. The user's message is the first element of the selected sample from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "# Get a random sample from the examples\n",
    "i = random.randint(0, len(examples) - 1)\n",
    "sample = examples[i]\n",
    "\n",
    "# Define the input data\n",
    "messages = { \"input_data\": { \n",
    "    \"input_string\": [{\"role\": \"user\", \"content\": sample[0]}], \n",
    "    \"parameters\": { \"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True, \"max_new_tokens\": 1000, }, }\n",
    "}\n",
    "\n",
    "# Write the input data to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode=\"w\") as temp:\n",
    "    json.dump(messages, temp)\n",
    "    temp_file_name = temp.name\n",
    "\n",
    "# Invoke the Phi-3 model and get the response\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=\"aistudio-nbrady-phi-3\",\n",
    "    deployment_name=\"phi-3-mini-4k-instruct\",\n",
    "    request_file=temp_file_name,\n",
    ")\n",
    "\n",
    "# Parse the response and add it to the input data\n",
    "response_json = json.loads(response)[\"output\"]\n",
    "response_dict = {'content': response_json, 'role': 'assistant'}\n",
    "messages['input_data']['input_string'].append(response_dict)\n",
    "\n",
    "# Print the updated input data\n",
    "print(json.dumps(messages[\"input_data\"][\"input_string\"],indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Chat Interface\n",
    "In this section, we are building a chat interface using Gradio, a Python library for creating UIs for machine learning models.\n",
    "\n",
    "First, we define a function `predict` that takes a message and a history of previous messages as input. This function prepares the input data for the Phi-3 model, invokes the model, and processes the model's response.\n",
    "\n",
    "```python\n",
    "def predict(message, history):\n",
    "    messages = {\n",
    "        \"input_data\": {\n",
    "            \"input_string\": [ ],\n",
    "            \"parameters\": { \"temperature\": 0.6, \"top_p\": 0.9, \"do_sample\": True, \"max_new_tokens\": 1000, }, } }\n",
    "    for user, assistant in history:\n",
    "        messages[\"input_data\"][\"input_string\"].append({\"content\": user, \"role\": \"user\"})\n",
    "        messages[\"input_data\"][\"input_string\"].append({\"content\": assistant, \"role\": \"assistant\"})\n",
    "    messages[\"input_data\"][\"input_string\"].append({\"content\": message, \"role\": \"user\"})\n",
    "    ...\n",
    "```\n",
    "We will use the same `tempile`, `invoke`, and `response` logic as we had before.\n",
    "\n",
    "After defining the `predict` function, we create a Gradio interface for it. This interface includes a textbox for the user to enter their message and a chatbot to display the conversation history. We also provide some example conversations to help users understand how to interact with the chatbot.\n",
    "\n",
    "> We also provided some example conversations (`examples` parameter in `gr.ChatInterface`) to help users understand how to interact with the chatbot. These examples were generated from an optional dataset. However, if you don't have such a dataset or prefer not to use it, you can simply remove the `examples` parameter from the `gr.ChatInterface` call.\n",
    "\n",
    "### Launch the Gradio interface\n",
    "```python\n",
    "gr.ChatInterface(\n",
    "    fn=predict,\n",
    "    textbox=gr.Textbox(\n",
    "        value=\"I am going to Paris, what should I see?\",\n",
    "        placeholder=\"Ask me anything...\",\n",
    "        scale=5,\n",
    "        lines=3,\n",
    "    ),\n",
    "    chatbot=gr.Chatbot(render_markdown=True),\n",
    "    examples=examples,\n",
    "    title=\"Phi-3: Tiny but mighty!\",\n",
    "    fill_height=True,\n",
    ").launch()\n",
    "```\n",
    "This process creates a user-friendly chat interface for the Phi-3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def predict(message, history):\n",
    "    # Define the input data\n",
    "    messages = {\n",
    "        \"input_data\": {\n",
    "            \"input_string\": [],\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 1000,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    for user, assistant in history:\n",
    "        messages[\"input_data\"][\"input_string\"].append({\"content\": user, \"role\": \"user\"})\n",
    "        messages[\"input_data\"][\"input_string\"].append({\"content\": assistant, \"role\": \"assistant\"})\n",
    "    messages[\"input_data\"][\"input_string\"].append({\"content\": message, \"role\": \"user\"})\n",
    "\n",
    "    # Write the input data to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode=\"w\") as temp:\n",
    "        json.dump(messages, temp)\n",
    "        temp_file_name = temp.name\n",
    "\n",
    "    # Invoke the Phi-3 model and get the response\n",
    "    response = workspace_ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=\"aistudio-nbrady-phi-3\",\n",
    "        deployment_name=\"phi-3-mini-4k-instruct\",\n",
    "        request_file=temp_file_name,\n",
    "    )\n",
    "\n",
    "    # Parse the response and add it to the input data\n",
    "    response_json = json.loads(response)[\"output\"]\n",
    "    response_dict = {\"content\": response_json, \"role\": \"assistant\"}\n",
    "    messages[\"input_data\"][\"input_string\"].append(response_dict)\n",
    "    \n",
    "    return response_json\n",
    "\n",
    "# Launch the Gradio interface\n",
    "gr.ChatInterface(\n",
    "    fn=predict,\n",
    "    textbox=gr.Textbox(\n",
    "        value=\"I am going to Paris, what should I see?\",\n",
    "        placeholder=\"Ask me anything...\",\n",
    "        scale=5,\n",
    "        lines=3,\n",
    "    ),\n",
    "    chatbot=gr.Chatbot(render_markdown=True, height=500),\n",
    "    examples=examples, # This is the optional list of examples we created earlier\n",
    "    title=\"Phi-3: Tiny but mighty!\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this blog post, we've walked through the process of building a user-friendly chat interface for the Phi-3 model using Gradio. This interface includes a textbox for the user to enter their message and a chatbot to display the conversation history. \n",
    "\n",
    "By creating this interface, we've made the Phi-3 model more accessible and easier to demo, allowing users to interact with it in a natural, conversational manner. This is a great example of how small learning models can be integrated into applications using natural language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
